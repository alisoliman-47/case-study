{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2cb70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import textwrap\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import s3fs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb9ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- PARAMETERS YOU MAY TUNE ---\n",
    "SUBSET_MAX_TXT = 80      # keep <100 as requested\n",
    "TOP_K = 12               # top results per query\n",
    "SUMMARY_MAX_WORDS = 70   # aim ~2-3 sentences\n",
    "KEYWORDS_TOP_N = 6\n",
    "\n",
    "# Models (small/CPU-friendly)\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# For summarization we'll try a distilled BART checkpoint fine-tuned for CNN/DM.\n",
    "# If unavailable locally, fallback to t5-small with the \"summarize: \" prefix.\n",
    "SUMM_MODEL_PRIMARY = \"sshleifer/distilbart-cnn-12-6\"\n",
    "SUMM_MODEL_FALLBACK = \"t5-small\"\n",
    "\n",
    "# Optional verifier (zero-shot)\n",
    "ZS_MODEL = \"facebook/bart-large-mnli\"  # ok on CPU for small batches\n",
    "\n",
    "# Themes for optional verifier\n",
    "THEMES = [\"Deep Learning\", \"Clinical Trial\", \"Traditional Methods\"]\n",
    "\n",
    "# Example input queries (from prompt)\n",
    "QUERIES = [\n",
    "    \"Adverse events with mRNA vaccines in pediatrics\",\n",
    "    \"Transformer-based models for protein folding\",\n",
    "    \"Clinical trial outcomes for monoclonal antibodies in oncology\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c40cca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s3fs.core.S3FileSystem object at 0x14a3e8830>\n"
     ]
    }
   ],
   "source": [
    "# Public S3 bucket per brief:\n",
    "BUCKET = \"pmc-oa-opendata\"\n",
    "FILELIST_KEY = \"oa_comm/txt/metadata/csv/oa_comm.filelist.csv\"\n",
    "TXT_PREFIX = \"oa_comm/txt/all/\"\n",
    "\n",
    "def get_fs():\n",
    "    # Anonymous access to the public bucket\n",
    "    # If your network blocks anon, set AWS creds and drop anon=True.\n",
    "    try:\n",
    "        return s3fs.S3FileSystem(anon=True)\n",
    "    except Exception:\n",
    "        return s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "fs = get_fs()\n",
    "print(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0df90a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,\n",
       " ['s3://pmc-oa-opendata/oa_comm/txt/all/PMC4893408.txt',\n",
       "  's3://pmc-oa-opendata/oa_comm/txt/all/PMC9760518.txt',\n",
       "  's3://pmc-oa-opendata/oa_comm/txt/all/PMC9924556.txt'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = get_fs()\n",
    "\n",
    "def sample_txt_paths(max_txt=SUBSET_MAX_TXT, seed=7, prefer_csv=True):\n",
    "    \"\"\"\n",
    "    Optimal strategy for this case study:\n",
    "      1) Try the official file-list CSV to collect oa_comm/txt/all/*.txt keys.\n",
    "      2) If that yields nothing (format/network issue), fall back to listing the TXT prefix.\n",
    "    Returns a deduplicated, seeded random sample of s3:// paths.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    candidates = []\n",
    "\n",
    "    # --- 1) CSV route (preferred) ---\n",
    "    if prefer_csv:\n",
    "        s3_csv = f\"s3://{BUCKET}/{FILELIST_KEY}\"\n",
    "        if fs.exists(s3_csv):\n",
    "            with fs.open(s3_csv, \"rb\") as f:\n",
    "                raw = f.read()\n",
    "\n",
    "            # Parse CSV with unknown schema; scan every column for txt/all/*.txt\n",
    "            df = pd.read_csv(io.BytesIO(raw), dtype=str, low_memory=False)\n",
    "            patt = re.compile(r\"(?:s3://pmc-oa-opendata/)?oa_comm/txt/all/[^\\s,]+?\\.txt\")\n",
    "\n",
    "            for col in df.columns:\n",
    "                s = df[col].dropna().astype(str)\n",
    "\n",
    "                # direct match (column already contains keys/uris)\n",
    "                direct = s[s.str.contains(r\"oa_comm/txt/all/.+\\.txt\", regex=True)].tolist()\n",
    "                if direct:\n",
    "                    candidates.extend(direct)\n",
    "                    continue\n",
    "\n",
    "                # otherwise, extract with regex from arbitrary text\n",
    "                for val in s:\n",
    "                    candidates.extend(patt.findall(val))\n",
    "\n",
    "    # Normalize to s3:// paths\n",
    "    s3_paths = [\n",
    "        p if str(p).startswith(\"s3://\")\n",
    "        else f\"s3://{BUCKET}/{str(p).lstrip('/')}\"\n",
    "        for p in pd.Series(candidates).dropna().drop_duplicates().tolist()\n",
    "    ]\n",
    "\n",
    "    # --- 2) Fallback: list the txt prefix directly ---\n",
    "    if not s3_paths:\n",
    "        prefix = f\"s3://{BUCKET}/{TXT_PREFIX}\"\n",
    "        # This can be large; we’ll just list and sample immediately.\n",
    "        listed = [p for p in fs.ls(prefix, detail=False) if str(p).endswith(\".txt\")]\n",
    "        s3_paths = listed\n",
    "\n",
    "    if not s3_paths:\n",
    "        raise RuntimeError(\"Could not find any TXT files via CSV or prefix listing.\")\n",
    "\n",
    "    # Seeded sample to keep <100 docs as required\n",
    "    k = min(len(s3_paths), max_txt)\n",
    "    idx = rng.choice(len(s3_paths), size=k, replace=False)\n",
    "    sample = [s3_paths[i] for i in idx]\n",
    "    return sample\n",
    "\n",
    "# Run the sampler\n",
    "sample_paths = sample_txt_paths()\n",
    "len(sample_paths), sample_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96d9b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ==== Front ISRN PainISRN PainISRN.PAINISRN Pain2314-4718Hindawi Publishing Corporation 10.1155/2013/726891Research ArticleCharacterization of the Visceral Antinociceptive Effect of Glial Glutamate Transporter GLT-1 Upregulation by Ceftriaxone Roman K.  1 Yang M.  2 Stephens Robert L. Jr. 1  * 1Department of Physiology and Cell Biology, The Ohio State University, 304 Hamilton Hall, 1645 Neil Avenue, Columbus, OH 43210, USA2Department of Gastroenterology, Daping Hospital, Third Military Medical University, Chongqing 400042, China*Robert L. Stephens Jr.: stephens.6@osu.eduAcademic Editors: A. Ul ...\n"
     ]
    }
   ],
   "source": [
    "assert sample_paths, \"sample_paths is empty\"\n",
    "with fs.open(sample_paths[0], \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    print(f.read(600)[:600].replace(\"\\n\", \" \"), \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a9bceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading & parsing: 100%|██████████| 80/80 [00:34<00:00,  2.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TITLE_PAT = re.compile(r\"^\\s*(title|article title)\\s*[:\\-]\\s*(.+)$\", re.I)\n",
    "ABSTRACT_PAT = re.compile(r\"^\\s*(abstract)\\s*[:\\-]?\\s*$\", re.I)\n",
    "\n",
    "@dataclass\n",
    "class Doc:\n",
    "    s3_path: str\n",
    "    title: str | None\n",
    "    abstract: str\n",
    "    raw: str\n",
    "\n",
    "def read_txt(path: str) -> str:\n",
    "    with fs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_title_and_abstract(raw: str) -> tuple[str | None, str]:\n",
    "    \"\"\"\n",
    "    Heuristics:\n",
    "    - Title: prefer a line like \"Title: ...\"\n",
    "    - Abstract: if a block begins with 'Abstract', take until a blank line;\n",
    "      else take the first ~1800 chars as a pseudo-abstract.\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in raw.splitlines()]\n",
    "\n",
    "    # Title\n",
    "    title = None\n",
    "    for ln in lines[:50]:\n",
    "        m = TITLE_PAT.match(ln)\n",
    "        if m:\n",
    "            title = m.group(2).strip()\n",
    "            break\n",
    "    if not title:\n",
    "        # Fallback: first non-empty line\n",
    "        for ln in lines:\n",
    "            if ln:\n",
    "                title = ln[:240]\n",
    "                break\n",
    "\n",
    "    # Abstract\n",
    "    abstract = \"\"\n",
    "    for i, ln in enumerate(lines):\n",
    "        if ABSTRACT_PAT.match(ln):\n",
    "            # collect until blank line or section break\n",
    "            chunk = []\n",
    "            for j in range(i+1, min(i+200, len(lines))):\n",
    "                if not lines[j]:  # stop at blank\n",
    "                    break\n",
    "                chunk.append(lines[j])\n",
    "            abstract = \" \".join(chunk).strip()\n",
    "            break\n",
    "\n",
    "    # fallback abstract\n",
    "    if not abstract:\n",
    "        abstract = raw.strip().split(\"\\n\\n\")[0]\n",
    "        abstract = abstract[:1800]\n",
    "\n",
    "    # clean whitespace\n",
    "    abstract = re.sub(r\"\\s+\", \" \", abstract).strip()\n",
    "    return title, abstract\n",
    "\n",
    "def load_docs(paths: list[str]) -> list[Doc]:\n",
    "    docs = []\n",
    "    for p in tqdm(paths, desc=\"Reading & parsing\"):\n",
    "        try:\n",
    "            raw = read_txt(p)\n",
    "            title, abstract = extract_title_and_abstract(raw)\n",
    "            if len(abstract) < 40:\n",
    "                continue\n",
    "            docs.append(Doc(p, title, abstract, raw))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return docs\n",
    "\n",
    "docs = load_docs(sample_paths)\n",
    "len(docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
